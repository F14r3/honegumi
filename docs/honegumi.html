<!DOCTYPE html>
<html>

<head>
    <title>Interactive Grid Example</title>
    </style>
    <link rel="stylesheet" href="_static/prism/prism.css">
    <link rel="stylesheet" href="_static/honegumi_style.css">
</head>

<body onload="updateText()">
    <script>
        var optionRows = [{"disable": false, "hidden": false, "name": "objective", "options": ["single", "multi"], "tooltip": "Choose between \u003ca href=\u0027curriculum/concepts/sobo-vs-mobo/sobo-vs-mobo.html\u0027\u003esingle and multi-objective optimization\u003c/a\u003e based on your project needs. Single objective optimization targets one primary goal (e.g. maximize the strength of a material), while multi-objective optimization considers several objectives simultaneously (e.g. maximize the strength of a material while minimizing synthesis cost). Select the option that best aligns with your optimization goals and problem complexity."}, {"disable": false, "hidden": false, "name": "model", "options": ["Default", "Fully Bayesian"], "tooltip": "Choose between \u003ca href=\u0027curriculum/concepts/freq-vs-bayes/freq-vs-bayes.html\u0027\u003efrequentist and fully bayesian\u003c/a\u003e implementations of the gaussian process (GP) surrogate model. The frequentist GP model, which is often the default in BO packages, offers efficiency and speed. The fully Bayesian GP models GP parameters as random variables through MCMC estimation, providing a deeper exploration of uncertainty. The fully bayesian treatment has historically provided better closed loop Bayesian optimization performance, but comes at the cost of higher computational demand. Consider your computational resources and the complexity of your optimization task when making your selection. This option asks you to choose between \u0027Default\u0027 and \u0027FullyBayesian\u0027, where, depending on the other options, \u0027Default\u0027 may be Noisy Gaussian Process Expected Improvement (NGPEI), Noisy Expected Hypervolume Improvement (NEHVI), etc."}, {"disable": false, "hidden": false, "name": "existing_data", "options": ["False", "True"], "tooltip": "Choose whether to fit the surrogate model to previous data before starting the optimization process. Including historical data may give your model a better starting place and potentially speed up convergence. Conversely, excluding existing data means starting the optimization from scratch, which might be preferred in scenarios where historical data could introduce bias or noise into the optimization process. Consider the relevance and reliability of your existing data when making your selection."}, {"disable": false, "hidden": false, "name": "custom_threshold", "options": ["False", "True"], "tooltip": "Choose whether to apply custom thresholds to objectives in a multi-objective optimization problem (e.g. a minimum acceptable strength requirement for a material). Setting a threshold on an objective guides the optimization algorithm to prioritize solutions that meet or exceed these criteria. Excluding thresholds enables greater exploration of the design space, but may produce sub-optimal solutions. Consider whether threshold values reflect the reality or expectations of your optimization task when selection this option."}];
        // E.g.,
        // var optionRows = [
        //     { "name": "row1", "options": ["A", "B"] },
        //     { "name": "row2", "options": ["C", "D", "E"] },
        //     { "name": "row3", "options": ["F", "G"] }
        // ];

        var scriptLookup = {"multi,Default,False,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n        obj2_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Default,False,True": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),\n        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Default,True,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nimport pandas as pd\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin_moo(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n        obj2_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Default,True,True": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nimport pandas as pd\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin_moo(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),\n        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Fully Bayesian,False,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIANMOO,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n        obj2_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Fully Bayesian,False,True": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIANMOO,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),\n        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Fully Bayesian,True,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\nimport pandas as pd\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin_moo(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIANMOO,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n        obj2_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "multi,Fully Bayesian,True,True": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\nimport pandas as pd\n\nobj1_name = \"branin\"\nobj2_name = \"branin_swapped\"\n\n\ndef branin_moo(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    # second objective has x1 and x2 swapped\n    y2 = float(\n        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)\n        + 10\n    )\n\n    return {obj1_name: y, obj2_name: y2}\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin_moo(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIANMOO,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),\n        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin_moo(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\npareto_results = ax_client.get_pareto_optimal_parameters()\n", "single,Default,False,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nobj1_name = \"branin\"\n\n\ndef branin(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    return y\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\n\nbest_parameters, metrics = ax_client.get_best_parameters()\n", "single,Default,False,True": "INVALID: The parameters you have selected are incompatible, either from not being implemented or being logically inconsistent.", "single,Default,True,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\n\nimport pandas as pd\n\nobj1_name = \"branin\"\n\n\ndef branin(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    return y\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\nax_client = AxClient()\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\n\nbest_parameters, metrics = ax_client.get_best_parameters()\n", "single,Default,True,True": "INVALID: The parameters you have selected are incompatible, either from not being implemented or being logically inconsistent.", "single,Fully Bayesian,False,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\n\nobj1_name = \"branin\"\n\n\ndef branin(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    return y\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIAN,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\n\nbest_parameters, metrics = ax_client.get_best_parameters()\n", "single,Fully Bayesian,False,True": "INVALID: The parameters you have selected are incompatible, either from not being implemented or being logically inconsistent.", "single,Fully Bayesian,True,False": "import numpy as np\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\n\nfrom ax.modelbridge.factory import Models\nfrom ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n\nimport pandas as pd\n\nobj1_name = \"branin\"\n\n\ndef branin(x1, x2):\n    y = float(\n        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2\n        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)\n        + 10\n    )\n\n    return y\n\n\n# Define the training data\n\n\nX_train = pd.DataFrame(\n    [\n        {\"x1\": -3.0, \"x2\": 5.0},\n        {\"x1\": 0.0, \"x2\": 6.2},\n        {\"x1\": 5.9, \"x2\": 2.0},\n        {\"x1\": 1.5, \"x2\": 2.0},\n        {\"x1\": 1.0, \"x2\": 9.0},\n    ]\n)\n\n# Define y_train (normally the values would be supplied directly instead of calculating here)\ny_train = [branin(row[\"x1\"], row[\"x2\"]) for _, row in X_train.iterrows()]\n\n# Define the number of training examples\nn_train = len(X_train)\n\n\ngs = GenerationStrategy(\n    steps=[\n        GenerationStep(\n            model=Models.SOBOL,\n            num_trials=4,  # https://github.com/facebook/Ax/issues/922\n            min_trials_observed=3,\n            max_parallelism=5,\n            model_kwargs={\"seed\": 999},\n            model_gen_kwargs={},\n        ),\n        GenerationStep(\n            model=Models.FULLYBAYESIAN,\n            num_trials=-1,\n            max_parallelism=3,\n            model_kwargs={\"num_samples\": 1024, \"warmup_steps\": 1024},\n        ),\n    ]\n)\n\nax_client = AxClient(generation_strategy=gs)\n\nax_client.create_experiment(\n    parameters=[\n        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 10.0]},\n    ],\n    objectives={\n        obj1_name: ObjectiveProperties(minimize=True),\n    },\n)\n\n# Add existing data to the AxClient\nfor i in range(n_train):\n    parameterization = X_train.iloc[i].to_dict()\n\n    ax_client.attach_trial(parameterization)\n    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])\n\n\nfor _ in range(19):\n\n    parameterization, trial_index = ax_client.get_next_trial()\n\n    # extract parameters\n    x1 = parameterization[\"x1\"]\n    x2 = parameterization[\"x2\"]\n\n    results = branin(x1, x2)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n\n\nbest_parameters, metrics = ax_client.get_best_parameters()\n", "single,Fully Bayesian,True,True": "INVALID: The parameters you have selected are incompatible, either from not being implemented or being logically inconsistent."};
        // E.g.,
        // var lookup = {
        //     "A,C,F": "ACF",
        //     "A,C,G": "INVALID",
        //     "A,D,F": "ADF",
        //     "A,D,G": "ADG",
        //     "A,E,F": "AEF",
        //     "A,E,G": "AEG",
        //     "B,C,F": "BCF",
        //     "B,C,G": "BCG",
        //     "B,D,F": "INVALID",
        //     "B,D,G": "BDG",
        //     "B,E,F": "INVALID",
        //     "B,E,G": "BEG"
        // };

        var preambleLookup = {"multi,Default,False,False": "", "multi,Default,False,True": "", "multi,Default,True,False": "", "multi,Default,True,True": "", "multi,Fully Bayesian,False,False": "", "multi,Fully Bayesian,False,True": "", "multi,Fully Bayesian,True,False": "", "multi,Fully Bayesian,True,True": "", "single,Default,False,False": "", "single,Default,False,True": "\u003ca href=\"https://colab.research.google.com/github/sgbaird/honegumi/blob/main/docs/generated_notebooks/ax/objective-single%2Bmodel-Default%2Bcustom_gen-False%2Bexisting_data-False%2Bcustom_threshold-True.ipynb\"\u003e\u003cimg alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/sgbaird/honegumi/tree/main/docs\\generated_scripts\\ax\\objective-single+model-Default+custom_gen-False+existing_data-False+custom_threshold-True.py\"\u003e\u003cimg alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open%20in%20GitHub-blue?logo=github\u0026labelColor=grey\"\u003e\u003c/a\u003e", "single,Default,True,False": "", "single,Default,True,True": "\u003ca href=\"https://colab.research.google.com/github/sgbaird/honegumi/blob/main/docs/generated_notebooks/ax/objective-single%2Bmodel-Default%2Bcustom_gen-False%2Bexisting_data-True%2Bcustom_threshold-True.ipynb\"\u003e\u003cimg alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/sgbaird/honegumi/tree/main/docs\\generated_scripts\\ax\\objective-single+model-Default+custom_gen-False+existing_data-True+custom_threshold-True.py\"\u003e\u003cimg alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open%20in%20GitHub-blue?logo=github\u0026labelColor=grey\"\u003e\u003c/a\u003e", "single,Fully Bayesian,False,False": "", "single,Fully Bayesian,False,True": "\u003ca href=\"https://colab.research.google.com/github/sgbaird/honegumi/blob/main/docs/generated_notebooks/ax/objective-single%2Bmodel-Fully%20Bayesian%2Bcustom_gen-True%2Bexisting_data-False%2Bcustom_threshold-True.ipynb\"\u003e\u003cimg alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/sgbaird/honegumi/tree/main/docs\\generated_scripts\\ax\\objective-single+model-Fully Bayesian+custom_gen-True+existing_data-False+custom_threshold-True.py\"\u003e\u003cimg alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open%20in%20GitHub-blue?logo=github\u0026labelColor=grey\"\u003e\u003c/a\u003e", "single,Fully Bayesian,True,False": "", "single,Fully Bayesian,True,True": "\u003ca href=\"https://colab.research.google.com/github/sgbaird/honegumi/blob/main/docs/generated_notebooks/ax/objective-single%2Bmodel-Fully%20Bayesian%2Bcustom_gen-True%2Bexisting_data-True%2Bcustom_threshold-True.ipynb\"\u003e\u003cimg alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/sgbaird/honegumi/tree/main/docs\\generated_scripts\\ax\\objective-single+model-Fully Bayesian+custom_gen-True+existing_data-True+custom_threshold-True.py\"\u003e\u003cimg alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open%20in%20GitHub-blue?logo=github\u0026labelColor=grey\"\u003e\u003c/a\u003e"};

        var invalidConfigs = [["single", "Default", "False", "True"], ["single", "Default", "True", "True"], ["single", "Fully Bayesian", "False", "True"], ["single", "Fully Bayesian", "True", "True"]];
        // E.g.,
        // var invalidConfigs = [
        //     ["B", "D", "F"],
        //     ["B", "E", "F"],
        //     ["A", "C", "G"]
        // ];
    </script>

    <form onsubmit="return false;">
        <div class="title-block">
            <div class="row align-items-center">
                <img src="_static/logo.png" alt="Logo" style="height: 2em; margin-right: 1em; vertical-align: middle;">
                <h2 style="margin-bottom: 0;">Honegumi</h2>
            </div>
        </div>

        <div class="row">
            <label class="row-label">objective:</label>

            <input type="radio" id="objective-single" name="objective" value="single" checked onclick="updateText()">
            <label class="row-option" for="objective-single">single</label>

            <input type="radio" id="objective-multi" name="objective" value="multi" onclick="updateText()">
            <label class="row-option" for="objective-multi">multi</label>

            <div class="tooltip-icon">&#9432;
                <span class="tooltip-text arrow">Choose between <a href='curriculum/concepts/sobo-vs-mobo/sobo-vs-mobo.html'>single and multi-objective optimization</a> based on your project needs. Single objective optimization targets one primary goal (e.g. maximize the strength of a material), while multi-objective optimization considers several objectives simultaneously (e.g. maximize the strength of a material while minimizing synthesis cost). Select the option that best aligns with your optimization goals and problem complexity.</span>
            </div>
        </div>

        <div class="row">
            <label class="row-label">model:</label>

            <input type="radio" id="model-Default" name="model" value="Default" checked onclick="updateText()">
            <label class="row-option" for="model-Default">Default</label>

            <input type="radio" id="model-Fully Bayesian" name="model" value="Fully Bayesian" onclick="updateText()">
            <label class="row-option" for="model-Fully Bayesian">Fully Bayesian</label>

            <div class="tooltip-icon">&#9432;
                <span class="tooltip-text arrow">Choose between <a href='curriculum/concepts/freq-vs-bayes/freq-vs-bayes.html'>frequentist and fully bayesian</a> implementations of the gaussian process (GP) surrogate model. The frequentist GP model, which is often the default in BO packages, offers efficiency and speed. The fully Bayesian GP models GP parameters as random variables through MCMC estimation, providing a deeper exploration of uncertainty. The fully bayesian treatment has historically provided better closed loop Bayesian optimization performance, but comes at the cost of higher computational demand. Consider your computational resources and the complexity of your optimization task when making your selection. This option asks you to choose between 'Default' and 'FullyBayesian', where, depending on the other options, 'Default' may be Noisy Gaussian Process Expected Improvement (NGPEI), Noisy Expected Hypervolume Improvement (NEHVI), etc.</span>
            </div>
        </div>

        <div class="row">
            <label class="row-label">existing_data:</label>

            <input type="radio" id="existing_data-False" name="existing_data" value="False" checked onclick="updateText()">
            <label class="row-option" for="existing_data-False">False</label>

            <input type="radio" id="existing_data-True" name="existing_data" value="True" onclick="updateText()">
            <label class="row-option" for="existing_data-True">True</label>

            <div class="tooltip-icon">&#9432;
                <span class="tooltip-text arrow">Choose whether to fit the surrogate model to previous data before starting the optimization process. Including historical data may give your model a better starting place and potentially speed up convergence. Conversely, excluding existing data means starting the optimization from scratch, which might be preferred in scenarios where historical data could introduce bias or noise into the optimization process. Consider the relevance and reliability of your existing data when making your selection.</span>
            </div>
        </div>

        <div class="row">
            <label class="row-label">custom_threshold:</label>

            <input type="radio" id="custom_threshold-False" name="custom_threshold" value="False" checked onclick="updateText()">
            <label class="row-option" for="custom_threshold-False">False</label>

            <input type="radio" id="custom_threshold-True" name="custom_threshold" value="True" onclick="updateText()">
            <label class="row-option" for="custom_threshold-True">True</label>

            <div class="tooltip-icon">&#9432;
                <span class="tooltip-text arrow">Choose whether to apply custom thresholds to objectives in a multi-objective optimization problem (e.g. a minimum acceptable strength requirement for a material). Setting a threshold on an objective guides the optimization algorithm to prioritize solutions that meet or exceed these criteria. Excluding thresholds enables greater exploration of the design space, but may produce sub-optimal solutions. Consider whether threshold values reflect the reality or expectations of your optimization task when selection this option.</span>
            </div>
        </div>

        <div class="row align-items-end">
            <p id="preamble"></p>
        </div>
        <div class="row">
        <div class="highlight highlight-python notranslate">
            <pre class="language-python" style="width: 102ch; overflow: auto; font-size: 12px;">
                <code id="text" style="border: none;"></code>
            </pre>
        </div>
        </div>
    </form>
    <script>
        function updateText() {
            var rows = document.querySelectorAll('input[type="radio"]:checked');
            // Remove strikethrough formatting from all labels except for the selected radio button's label
            // While not strictly necessary, it prevents a possible "disappear/appear" effect
            var labels = document.querySelectorAll('label');
            labels.forEach(function (label) {
                if (label.htmlFor !== rows.id) {
                    label.innerHTML = label.textContent;
                }
            });
            var key = Array.from(rows, function (row) { return row.value; }).join(',');
            var rendered = scriptLookup[key];
            var preamble = preambleLookup[key];

            // Update the GitHub link
            document.getElementById("preamble").innerHTML = preamble
            document.getElementById("text").innerHTML = "\n" + rendered;
            var currentConfig = Array.from(rows, function (row) { return row.value; });

            // Generate all possible configurations that deviate exactly by one option from the current configuration
            var possibleDeviatingConfigs = [];
            for (var i = 0; i < optionRows.length; i++) {
                var options = optionRows[i]['options'];
                for (var j = 0; j < options.length; j++) {
                    var option = options[j];
                    var deviation = currentConfig.slice();
                    deviation[i] = option;
                    // Skip the configuration if it's the same as the current configuration
                    if (JSON.stringify(deviation) !== JSON.stringify(currentConfig)) {
                        possibleDeviatingConfigs.push(deviation);
                    }
                }
            }
            // Remove duplicates
            possibleDeviatingConfigs = possibleDeviatingConfigs.filter(function (config, index) {
                return possibleDeviatingConfigs.indexOf(config) === index;
            });

            // Find the invalid configurations that match the deviating configurations
            var deviatingConfigs = [];
            for (var i = 0; i < invalidConfigs.length; i++) {
                var config = invalidConfigs[i];
                if (possibleDeviatingConfigs.some(function (deviation) {
                    return deviation.every(function (value, index) {
                        return value === config[index];
                    });
                })) {
                    deviatingConfigs.push(config);
                }
            }
            // Track the one option that was the deviation for each deviating config
            var deviatingOptions = deviatingConfigs.map(function (config) {
                var deviatingIndex = config.findIndex(function (value, index) {
                    return value !== currentConfig[index];
                });
                return optionRows[deviatingIndex]['name'] + '-' + config[deviatingIndex];
            });

            // Check if the current configuration is invalid
            var isInvalid = invalidConfigs.some(function (config) {
                return config.every(function (value, index) {
                    return value === currentConfig[index];
                });
            });

            // If the current configuration is invalid, add all selected options to the list of deviatingOptions
            if (isInvalid) {
                var currentConfigWithName = currentConfig.map(function (value, index) {
                    return optionRows[index]['name'] + '-' + value;
                });
                deviatingOptions = deviatingOptions.concat(currentConfigWithName);
            }

            // Logging (OK to comment out)
            console.group('Current Config');
            console.table(currentConfig);
            console.group('Deviations');
            console.group('Possible Deviating Configs');
            console.table(possibleDeviatingConfigs);
            console.groupEnd();
            console.group('Deviating Configs');
            console.table(deviatingConfigs);
            console.groupEnd();
            console.group('Deviating Options');
            console.table(deviatingOptions);
            console.groupEnd();
            console.groupEnd();

            // Add strikethrough for each deviating option
            deviatingOptions.forEach(function (option) {
                var label = document.querySelector('label[for="' + option + '"]');
                if (label) {
                    label.innerHTML = '<s>' + label.innerHTML + '</s>';
                }
            });
            Prism.highlightAll();
        }
    </script>
    <script src="_static/prism/prism.js"></script>
</body>

</html>

<!-- // Generate all possible configurations that deviate by zero or one option from the current configuration
// start with current config
var possibleDeviatingConfigs = [currentConfig.slice()]; -->
